{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人工知能特論2 課題 Deep Fake Challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nf4k-\\.pyenv\\pyenv-win\\versions\\3.9.6\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Shape:\n",
    "    C: int\n",
    "    H: int\n",
    "    W: int\n",
    "    N: int\n",
    "\n",
    "    def tuple(self):\n",
    "        return (self.C, self.H, self.W, self.N)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # fixed seed\n",
    "    seed = 111\n",
    "    batch_size: int = 32\n",
    "\n",
    "    # load the checkpoint from the start_epoch\n",
    "    start_epoch: int = 0\n",
    "    n_epochs: int = 10\n",
    "\n",
    "    # - /\n",
    "    #   VideoFrames/{id}_{frame}.jpg\n",
    "    #   FakeFaces_test.csv: {file_name}, {REAL/FAKE}\n",
    "    #   FakeFaces_train.csv: {file_name}, {REAL/FAKE}\n",
    "    dataset_path = Path('./dataset/FakeFaces')\n",
    "    # -1 to use all data\n",
    "    dataset_size: int = -1\n",
    "    # (C, H, W, N) = (3, 360, 640, 60)\n",
    "    data_shape = Shape(3, 360, 640, 60)\n",
    "    \n",
    "    exp_id: str = 'deepfake_detect_attn_0715'\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x253b6c66e30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed((config.seed))\n",
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logdir=runs/deepfake_detect_attn_0715\n"
     ]
    }
   ],
   "source": [
    "def setup_tensorboard(id):\n",
    "    # template = \"%Y-%m-%d_%H-%M-%S\"\n",
    "    print(f'logdir=runs/{id}')\n",
    "    writer = SummaryWriter(f'runs/{id}')\n",
    "    return writer\n",
    "\n",
    "writer = setup_tensorboard(config.exp_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, Dataloader\n",
    "- video_id: 0..=399, 欠落有り\n",
    "- 640x360, 3 channels, 60 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>289_065.jpg</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>356_295.jpg</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356_150.jpg</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002_275.jpg</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>385_290.jpg</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     File Name Label\n",
       "0  289_065.jpg  REAL\n",
       "1  356_295.jpg  REAL\n",
       "2  356_150.jpg  REAL\n",
       "3  002_275.jpg  REAL\n",
       "4  385_290.jpg  REAL"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(config.dataset_path / 'FakeFaces_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 360, 640]) 0\n",
      "train_dataset_size: 21090, valid_dataset_size: 1110\n"
     ]
    }
   ],
   "source": [
    "class DeepFakeDetectDataset(Dataset):\n",
    "    def __init__(self, dataset_dir: Path):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        # \"File Name\", \"Label\"\n",
    "        self.df = pd.read_csv(dataset_dir / 'FakeFaces_train.csv') \\\n",
    "            .replace({ 'REAL': 0, 'FAKE': 1 })\n",
    "        self.len = len(self.df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_name, label = row\n",
    "        file_path = self.dataset_dir / 'VideoFrames' / file_name\n",
    "        im = read_image(str(file_path)) / 255.\n",
    "        return im, label\n",
    "\n",
    "dataset = DeepFakeDetectDataset(config.dataset_path)\n",
    "im, label = dataset[0]\n",
    "print(im.shape, label)\n",
    "\n",
    "tra_val_ratio = 0.95\n",
    "train_dataset_size = int(len(dataset) * tra_val_ratio)\n",
    "val_dataset_size = len(dataset) - train_dataset_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_dataset_size, val_dataset_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "print(f'train_dataset_size: {train_dataset_size}, valid_dataset_size: {val_dataset_size}')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from typing import Optional\n",
    "\n",
    "def read_images_as_tensor(shape: Shape, dir: str, video_id: str) -> Optional[torch.Tensor]:\n",
    "    tensors = []\n",
    "    frame_paths = glob(f'{dir}/{video_id}_*.jpg')\n",
    "    if len(frame_paths) != shape.N:\n",
    "        return None\n",
    "\n",
    "    for frame in sorted(frame_paths):\n",
    "        img = Image.open(frame)\n",
    "        # (C, H, W)\n",
    "        img_tensor = to_tensor(img)\n",
    "        tensors.append(img_tensor)\n",
    "    # (C, H, W, N)\n",
    "    return torch.stack(tensors, dim=3)\n",
    "\n",
    "# video_id = 0\n",
    "# video_tensor = read_images_as_tensor(config.data_shape, f'{config.dataset_path}/VideoFrames', f'{video_id:03d}')\n",
    "# assert(video_tensor.shape == config.data_shape.tuple())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_with_bn_relu(\n",
    "    in_channels: int, \n",
    "    out_channels: int, \n",
    "    kernel_size: int, \n",
    "    stride: int, \n",
    "    padding: int\n",
    ") -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class SABlock(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super(SABlock, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_with_bn_relu(in_channels, in_channels, 3, 1, 1),\n",
    "            nn.Conv2d(in_channels, 1, 1, 1, 0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x) * x\n",
    "\n",
    "\n",
    "# サンプルコード参考\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        (C, H, W, N) = config.data_shape.tuple()\n",
    "        self.net = nn.Sequential(\n",
    "            conv_with_bn_relu(C, 32, 5, 5, 0), # size: 1/5\n",
    "            conv_with_bn_relu(32, 64, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=2), # 1/10\n",
    "            SABlock(64),\n",
    "            conv_with_bn_relu(64, 128, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=2), # 1/20\n",
    "            conv_with_bn_relu(128, 128, 3, 1, 1),\n",
    "            nn.MaxPool2d(kernel_size=2), # 1/40\n",
    "            nn.Flatten(),\n",
    "            # size is now 1/40\n",
    "            nn.Linear(128 * (H // 40) * (W // 40) , 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, N),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(model: Model, optimizer, config: Config, epoch: int):\n",
    "    print(f'save models @ epoch={epoch}')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, f'models/{config.exp_id}_{epoch}.pt')\n",
    "\n",
    "\n",
    "def load_checkpoint(config: Config, start_epoch: int):\n",
    "    print(f'load models @ epoch={start_epoch}')\n",
    "    checkpoint = torch.load(f'models/{config.exp_id}_{start_epoch}.pt')\n",
    "\n",
    "    config.start_epoch = checkpoint['epoch'] + 1\n",
    "    model = Model(config).to(device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(\n",
    "    writer: SummaryWriter,\n",
    "    model: Model,\n",
    "    optimizer,\n",
    "    dataloader: DataLoader,\n",
    "    validation_dataloader: DataLoader,\n",
    "    config: Config,\n",
    "):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(f'criterion: {criterion}')\n",
    "\n",
    "    for epoch in range(config.n_epochs):\n",
    "        now = datetime.datetime.now\n",
    "        print(f'[{now()}] Epoch {epoch}')\n",
    "\n",
    "        # train\n",
    "        loss_avg = 0.\n",
    "        model.train()\n",
    "        for i, (x, y) in enumerate(tqdm(dataloader)):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step = epoch * len(dataloader) + i\n",
    "            loss_avg += loss.item()\n",
    "            writer.add_scalar('train/loss', loss.item(), step)\n",
    "        \n",
    "        loss_avg /= len(dataloader)\n",
    "        print(f'[{now()}] Epoch {epoch} tra/loss: {loss_avg:.2f}')\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        loss_avg = 0.\n",
    "        with torch.inference_mode():\n",
    "            for i, (x, y) in enumerate(tqdm(validation_dataloader)):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y)\n",
    "                step = epoch * len(validation_dataloader) + i\n",
    "                loss_avg += loss.item()\n",
    "                writer.add_scalar('validation/loss', loss.item(), step)\n",
    "\n",
    "        loss_avg /= len(validation_dataloader)\n",
    "        print(f'[{now()}] Epoch {epoch} val/loss: {loss_avg:.2f}')\n",
    "\n",
    "        save_checkpoint(model, optimizer, config, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_cache():\n",
    "    # empty cache\n",
    "    torch.cuda.empty_cache()\n",
    "    # print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion: CrossEntropyLoss()\n",
      "[2022-07-15 13:43:59.640790] Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [03:39<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-15 13:47:39.118202] Epoch 0 tra/loss: 6.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:10<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-15 13:47:49.233689] Epoch 0 val/loss: 4.88\n",
      "save models @ epoch=0\n",
      "[2022-07-15 13:47:49.930801] Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [02:28<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-15 13:50:18.722059] Epoch 1 tra/loss: 1.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-15 13:50:25.172107] Epoch 1 val/loss: 1.49\n",
      "save models @ epoch=1\n",
      "[2022-07-15 13:50:25.871762] Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [02:35<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-15 13:53:01.402935] Epoch 2 tra/loss: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:08<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-15 13:53:09.419449] Epoch 2 val/loss: 0.58\n",
      "save models @ epoch=2\n",
      "[2022-07-15 13:53:10.105015] Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [02:33<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-15 13:55:44.069585] Epoch 3 tra/loss: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-15 13:55:50.792336] Epoch 3 val/loss: 0.51\n",
      "save models @ epoch=3\n",
      "[2022-07-15 13:55:51.485112] Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 208/660 [00:49<01:36,  4.67it/s]"
     ]
    }
   ],
   "source": [
    "clean_cache()\n",
    "model = Model(config).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train(writer, model, optimizer, train_dataloader, valid_dataloader, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3b5149f69c5c71e17d563646443db8801bb1e0a6ffc5a9144cdfc320ca09ead"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
